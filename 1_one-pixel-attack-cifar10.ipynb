{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# One Pixel Attack Tutorial\n",
    "## Part 1 - Cifar10\n",
    "\n",
    "### Dan Kondratyuk\n",
    "### September 15, 2019"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](images/pred2.png \"All it takes is one pixel\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook will demonstrate the one pixel attack with a few different convolutional neural network models. By using differential evolution, we find a special pixel that can modify a target image such that the network misclassifies the image (which it previously correctly classified).\n",
    "\n",
    "In theory, we want models that don't get fooled by such tiny changes. Especially in images, it is undesirable to have a small alteration in the input result in a drastic change in the output. However, even the most accurate neural networks are susceptible to this type of attack.\n",
    "\n",
    "To read more about it, see [the original paper](https://arxiv.org/abs/1710.08864), or the authors' [official repo](https://github.com/Carina02/One-Pixel-Attack).\n",
    "\n",
    "Let's get started."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ensure that you have `numpy`, `pandas`, `scipy`, `matplotlib`, `tensorflow-gpu`, and `keras` installed.\n",
    "\n",
    "Alternatively, you may [run this notebook in Google Colab](https://colab.research.google.com/drive/1Zq1kGP9C7i-70-SXyuEEaqYngtyQZMn7). Note: colab allows you to run this notebook on GPU, free of charge. Simply select \"GPU\" in the Accelerator drop-down in Notebook Settings (either through the Edit menu or the command palette at cmd/ctrl-shift-P)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Anything above 2.10 is not supported on the GPU on Windows Native\n",
    "#!pip install \"tensorflow<2.11\" "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install tensorflow-gpu==2.10 --user"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\"\n",
    "gpus = tf.config.experimental.list_physical_devices('GPU')\n",
    "if gpus:\n",
    "    # Currently, memory growth needs to be the same across GPUs\n",
    "    for gpu in gpus:\n",
    "        tf.config.experimental.set_memory_growth(gpu, True)\n",
    "    logical_gpus = tf.config.experimental.list_logical_devices('GPU')\n",
    "    print(len(gpus), \"Physical GPUs,\", len(logical_gpus), \"Logical GPUs\")\n",
    "tf.test.is_gpu_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-09T06:58:25.037882Z",
     "start_time": "2023-05-09T06:58:24.997879800Z"
    }
   },
   "outputs": [],
   "source": [
    "# If running in Google Colab, import files\n",
    "try:\n",
    "    import google.colab\n",
    "    in_colab = True\n",
    "except:\n",
    "    in_colab = False\n",
    "\n",
    "if in_colab:\n",
    "    !git clone https://github.com/Hyperparticle/one-pixel-attack-keras.git\n",
    "    !mv -v one-pixel-attack-keras/* .\n",
    "    !rm -rf one-pixel-attack-keras\n",
    "\n",
    "\n",
    "import helper\n",
    "\n",
    "# Python Libraries\n",
    "%matplotlib inline\n",
    "import pickle\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib\n",
    "from keras.datasets import cifar10\n",
    "from keras import backend as K\n",
    "import os\n",
    "\n",
    "# Custom Networks\n",
    "from networks.lenet import LeNet\n",
    "from networks.pure_cnn import PureCnn\n",
    "from networks.network_in_network import NetworkInNetwork\n",
    "from networks.resnet import ResNet\n",
    "from networks.densenet import DenseNet\n",
    "from networks.wide_resnet import WideResNet\n",
    "from networks.capsnet import CapsNet\n",
    "\n",
    "# Helper functions\n",
    "from differential_evolution import differential_evolution\n",
    "\n",
    "matplotlib.style.use('ggplot')\n",
    "np.random.seed(100)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Dataset"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For this attack, we will use the [Cifar10 dataset](https://www.cs.toronto.edu/~kriz/cifar.html) packaged by Keras. The task of the dataset is to correctly classify a 32x32 pixel image in 1 of 10 categories (e.g., bird, deer, truck).\n",
    "\n",
    "The code below will load the Cifar10 dataset. Keras will need to download the dataset if it is not cached locally already."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-09T06:51:36.954098600Z",
     "start_time": "2023-05-09T06:51:36.944108200Z"
    }
   },
   "outputs": [],
   "source": [
    "(x_train, y_train), (x_test, y_test) = cifar10.load_data()\n",
    "len(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class_names = ['airplane', 'automobile', 'bird', 'cat', 'deer', 'dog', 'frog', 'horse', 'ship', 'truck']"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can access and display any image in the dataset by its index. For instance, here is a horse."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_id = 4 # Image index in the test set\n",
    "helper.plot_image(x_test[image_id])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Image Perturbation"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To begin, we need a function to modify one or more pixels in an image. \n",
    "\n",
    "We can define the perturbation of a pixel as a 5-tuple \n",
    "\n",
    "$$\\textbf{x} = (x, y, r, g, b)$$\n",
    "\n",
    "where $x, y$ are the coordinates of the pixel from 0 to 31, and $r,g,b$ are the red, green, and blue values from 0 to 255. Then multiple perturbations can simply be a concatenation of these tuples:\n",
    "\n",
    "$$X = (x_1, y_1, r_1, g_1, b_1, x_2, y_2, r_2, g_2, b_2, ...)$$\n",
    "\n",
    "We could instead use an array of tuples, but the optimization algorithm we will use requires it to be a flat 1-d vector.\n",
    "\n",
    "Then the function to perturb an image can take as an input the image and $X$, and output a copy of the image with each pixel at $x_i, y_i$ modified to have the color $r_i, g_i, b_i$. To speed up computation, we will batch together an array of $X$ perturbations, denoted $X_S$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def perturb_image(xs, img):\n",
    "    # If this function is passed just one perturbation vector,\n",
    "    # pack it in a list to keep the computation the same\n",
    "    if xs.ndim < 2:\n",
    "        xs = np.array([xs])\n",
    "    \n",
    "    # Copy the image n == len(xs) times so that we can \n",
    "    # create n new perturbed images\n",
    "    tile = [len(xs)] + [1]*(xs.ndim+1)\n",
    "    imgs = np.tile(img, tile)\n",
    "    \n",
    "    # Make sure to floor the members of xs as int types\n",
    "    xs = xs.astype(int)\n",
    "    \n",
    "    for x,img in zip(xs, imgs):\n",
    "        # Split x into an array of 5-tuples (perturbation pixels)\n",
    "        # i.e., [[x,y,r,g,b], ...]\n",
    "        pixels = np.split(x, len(x) // 5)\n",
    "        for pixel in pixels:\n",
    "            # At each pixel's x,y position, assign its rgb value\n",
    "            x_pos, y_pos, *rgb = pixel\n",
    "            print(x_pos,y_pos)\n",
    "            img[x_pos, y_pos] = rgb\n",
    "    \n",
    "    return imgs"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can modify the pixels of any image we want.\n",
    "\n",
    "Let's modify our horse image by making pixel (16,16) yellow."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_id = 99 # Image index in the test set\n",
    "pixel = np.array([16, 16, 255, 255, 0]) # pixel = x,y,r,g,b\n",
    "image_perturbed = perturb_image(pixel, x_test[image_id])[0]\n",
    "\n",
    "\n",
    "helper.plot_image(image_perturbed)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Models"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To demonstrate the attack, we need some neural network models trained on the Cifar10 dataset. We will now load some pretrained models, which can be found in the `networks/models` directory.\n",
    "\n",
    "It is recommended to use Keras with a GPU enabled. If you're [running in Google Colab](https://colab.research.google.com/drive/1Zq1kGP9C7i-70-SXyuEEaqYngtyQZMn7), you can enable a GPU instance by selecting `Runtime > Change runtime type > Hardware accelerator > GPU` (you will need to re-run all cells). The code below can be used to check (if using TensorFlow)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Should output /device:GPU:0\n",
    "# K.tensorflow_backend._get_available_gpus()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are two models included in this repository, `lenet` and `resnet` which will be loaded from disk automatically.\n",
    "\n",
    "Optionally, you may [download the larger, more accurate models](https://www.dropbox.com/sh/dvatkpjl0sn79kn/AAC9L4puJ_sdFUkDZfr5SFkLa?dl=0) (e.g., Capsule Network, DenseNet, etc.). Make sure to copy the models into the `networks/models/` directory. Then uncomment the lines below and run the cell to load the models of your choosing."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below we can modify a pixel in an image and see how the confidence of the model changes. In almost all cases, the confidence will not change. However, for very special cases it will change drastically."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we will demonstrate two variants of the one pixel attack: untargeted and targeted."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The objective of an untargeted attack is to cause a model to misclassify an image. This means we want to perturb an image as to minimize the confidence probability of the correct classification category and maximize the sum of the probabilities of all other categories.\n",
    "\n",
    "The objective of a targeted attack is to cause a model to classify an image as a given  target class. We want to perturb an image as to maximize the probability of a class of our own choosing."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Success Criterion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def attack_success(x, img, target_class, model, targeted_attack=False, verbose=False):\n",
    "    # Perturb the image with the given pixel(s) and get the prediction of the model\n",
    "    attack_image = perturb_image(x, img)\n",
    "\n",
    "    confidence = model.predict(attack_image)[0]\n",
    "    predicted_class = np.argmax(confidence)\n",
    "    \n",
    "    # If the prediction is what we want (misclassification or \n",
    "    # targeted classification), return True\n",
    "    if verbose:\n",
    "        print('Confidence:', confidence[target_class])\n",
    "    if ((targeted_attack and predicted_class == target_class) or\n",
    "        (not targeted_attack and predicted_class != target_class)):\n",
    "        return True\n",
    "    # NOTE: return None otherwise (not False), due to how Scipy handles its callback function"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we demonstrate the usage of the success criterion function. It's nearly identical to `predict_class()` as before, but also decides the success of the attack. For purposes of demonstration we assume an untargeted attack."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_id = 541\n",
    "pixel = np.array([17, 18, 185, 36, 215])\n",
    "model = resnet\n",
    "\n",
    "true_class = y_test[image_id, 0]\n",
    "prior_confidence = model.predict_one(x_test[image_id])[true_class]\n",
    "success = attack_success(pixel, x_test[image_id], true_class, model, verbose=True)\n",
    "\n",
    "print('Prior confidence', prior_confidence)\n",
    "print('Attack success:', success == True)\n",
    "helper.plot_image(perturb_image(pixel, x_test[image_id])[0])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Attack Function"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we arrive at the attack itself: how do we find the pixels that will result in a successful attack? First, formulate it as an optimization problem: in an untargeted attack, minimize the confidence of the correct class, and in a targeted attack, maximize the confidence of a target class. This is precisely our `predict_class` function.\n",
    "\n",
    "When performing black-box optimizations such as the one pixel attack, it can be very difficult to find an efficient gradient-based optimization that will work for the problem. It would be nice to use an optimization algorithm that can find good solutions without relying on the smoothness of the function. In our case, we have discrete integer positions ranging from 0 to 31 and color intensities from 0 to 255, so the function is expected to be jagged.\n",
    "\n",
    "For that, we use an algorithm called [differential evolution](https://en.wikipedia.org/wiki/Differential_evolution). Here's an example of differential evolution optimizing the [Ackley function](https://en.wikipedia.org/wiki/Ackley_function) (if you're using Google Colab, run the code cell below):\n",
    "\n",
    "<br>\n",
    "\n",
    "![Ackley GIF](images/Ackley.gif)\n",
    "\n",
    "<br>\n",
    "\n",
    "Differential evolution is a type of evolutionary algorithm where a population of candidate solutions generate offspring which compete with the rest of the population each generation according to their fitness. Each candidate solution is represented by a vector of real numbers which are the inputs to the function we would like to minimize. The lower the output of this function, the better the fitness. The algorithm works by initializing a (usually random) population of vectors, generating new offspring vectors by combining (mutating) individuals in the population, and replacing worse-performing individuals with better candidates.\n",
    "\n",
    "In the context of the one pixel attack, our input will be a flat vector of pixel values:\n",
    "\n",
    "$$X = (x_1, y_1, r_1, g_1, b_1, x_2, y_2, r_2, g_2, b_2, ...)$$\n",
    "\n",
    "These will be encoded as floating-point values, but will be floored back into integers to calculate image perturbations. First we generate a random population of $n$ perturbations\n",
    "\n",
    "$$\\textbf{P} = (X_1, X_2, \\dots, X_n)$$\n",
    "\n",
    "Then, on each iteration we calculate $n$ new mutant children using the formula\n",
    "\n",
    "$$X_i = X_{r1} + F (X_{r2} - X_{r3})$$\n",
    "\n",
    "such that\n",
    "\n",
    "$$r1 \\neq r2 \\neq r3$$\n",
    "\n",
    "where $r1,r2,r3$ are random indices into our population $\\textbf{P}$, and $F = 0.5$ is a mutation parameter. Basically, we pick 3 random individuals from the previous generation and recombine them to make a new candidate solution. If this candidate $X_i$ gives a lower minimum at position $i$ (i.e., the attack is closer to success), replace the old $X_i$ with this new one. This process repeats for several iterations until our stopping criterion, `attack_success`, which is when we find an image that successfully completes the attack.\n",
    "\n",
    "<br>\n",
    "\n",
    "See [this excellent tutorial post](https://pablormier.github.io/2017/09/05/a-tutorial-on-differential-evolution-with-python/) on how differential evolution works in greater detail. \n",
    "\n",
    "We will use a [slight modification](differential_evolution.py) of [Scipy's implementation of differential evolution](https://docs.scipy.org/doc/scipy-0.17.0/reference/generated/scipy.optimize.differential_evolution.html) to utilize GPU parallelism by batching predictions together."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "tlO00HlxbB0k"
   },
   "outputs": [],
   "source": [
    "# Run this cell if you are using Google Colab to see the Ackley GIF\n",
    "if in_colab:\n",
    "    from IPython.display import Image\n",
    "    with open('images/Ackley.gif','rb') as file:\n",
    "        display(Image(file.read()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-15T18:47:54.731779Z",
     "start_time": "2023-05-15T18:47:54.727658Z"
    },
    "colab": {},
    "colab_type": "code",
    "id": "7rlzqXpqSgrW"
   },
   "outputs": [],
   "source": [
    "import helper\n",
    "def attack(img_id, model, target=None, pixel_count=1, \n",
    "           maxiter=75, popsize=400, verbose=False, attack_img = False):\n",
    "    # Change the target class based on whether this is a targeted attack or not\n",
    "    targeted_attack = target is not None\n",
    "    target_class = target if targeted_attack else y_train[img_id, 0]\n",
    "    \n",
    "    # Define bounds for a flat vector of x,y,r,g,b values\n",
    "    # For more pixels, repeat this layout\n",
    "    bounds = [(0,32), (0,32), (0,256), (0,256), (0,256)] * pixel_count\n",
    "    \n",
    "    # Population multiplier, in terms of the size of the perturbation vector x\n",
    "    popmul = max(1, popsize // len(bounds))\n",
    "    \n",
    "    # Format the predict/callback functions for the differential evolution algorithm\n",
    "    def predict_fn(xs):\n",
    "        return predict_classes(xs, x_train[img_id], target_class, \n",
    "                               model, target is None)\n",
    "    \n",
    "    def callback_fn(x, convergence):\n",
    "        return attack_success(x, x_train[img_id], target_class, \n",
    "                              model, targeted_attack, verbose)\n",
    "    \n",
    "    # Call Scipy's Implementation of Differential Evolution\n",
    "    attack_result = differential_evolution(\n",
    "        predict_fn, bounds, maxiter=maxiter, popsize=popmul,\n",
    "        recombination=1, atol=-1, callback=callback_fn, polish=False)\n",
    "\n",
    "    # Calculate some useful statistics to return from this function\n",
    "    attack_image = perturb_image(attack_result.x, x_train[img_id])[0]\n",
    "    prior_probs = model.predict_one(x_train[img_id])\n",
    "    predicted_probs = model.predict_one(attack_image)\n",
    "    predicted_class = np.argmax(predicted_probs)\n",
    "    actual_class = y_train[img_id, 0]\n",
    "    success = predicted_class != actual_class\n",
    "    cdiff = prior_probs[actual_class] - predicted_probs[actual_class]\n",
    "\n",
    "    # Show the best attempt at a solution (successful or not)\n",
    "    # helper.plot_image(attack_image, actual_class, class_names, predicted_class)\n",
    "    if attack_img :\n",
    "        return [model.name, pixel_count, img_id, actual_class, predicted_class, success, cdiff, prior_probs, predicted_probs, attack_result.x, attack_image]\n",
    "    else:\n",
    "        return [model.name, pixel_count, img_id, actual_class, predicted_class, success, cdiff, prior_probs, predicted_probs, attack_result.x]\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Untargeted Attack"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's look at one iteration of the untargeted attack. Here we will demonstrate a successful attack an image of a frog with the `resnet` model. We should see the confidence in the true class drop after several iterations.\n",
    "\n",
    "Try to see if you can successfully attack other images/models. The more pixels we are allowed to modify, the more likely it is we are to find a solution for any given image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_id = 20\n",
    "pixels = 1 # Number of pixels to attack\n",
    "# model = lenet\n",
    "model = resnet\n",
    "# model = net_in_net\n",
    "# model = densenet\n",
    "\n",
    "_ = attack(image_id, model, pixel_count=pixels, verbose=True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Targeted Attack"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the targeted attack, we can choose which class we want a model to classify an image as. The task is much harder for the targeted attack, as we constrain the misclassification to a given class rather than any class that's not the correct one. We should see the confidence in the target class rise after several iterations.\n",
    "\n",
    "Below we try to cause the `lenet` to classify an image of a `ship` as an `automobile`. Try to change the parameters and see what happens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_id = 108\n",
    "target_class = 1 # Integer in range 0-9\n",
    "pixels = 3\n",
    "model = resnet\n",
    "\n",
    "print('Attacking with target', class_names[target_class])\n",
    "_ = attack(image_id, model, target_class, pixel_count=pixels, verbose=True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Collect Results"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Armed with all the necessary tools to conduct a one pixel attack, the final step is to collect relevant statistics on the targeted and untargeted attack. The relevant data points are what percentage of images were we able to successfully attack for a given model, and how the number of pixels affect this percentage.\n",
    "\n",
    "We will loop through every combination of all models, perturbations of 1,3,5 pixels, images, and target classes (for the targeted attack). This will take a lot of computational resources and time, so [skip to the statistics section](#Attack-Statistics) if that's not your idea of fun."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "def attack_all(models, samples=500, pixels=(1,3,5), targeted=False, \n",
    "               maxiter=75, popsize=400, verbose=False):\n",
    "    results = []\n",
    "    for model in models:\n",
    "        model_results = []\n",
    "        valid_imgs = correct_imgs[correct_imgs.name == model.name].img\n",
    "        img_samples = np.random.choice(valid_imgs, samples, replace=False)\n",
    "        \n",
    "        for pixel_count in pixels:\n",
    "            for i, img_id in enumerate(img_samples):\n",
    "                print('\\n', model.name, '- image', img_id, '-', i+1, '/', len(img_samples))\n",
    "                targets = [None] if not targeted else range(10)\n",
    "                \n",
    "                for target in targets:\n",
    "                    if targeted:\n",
    "                        print('Attacking with target', class_names[target])\n",
    "                        if target == y_test[img_id, 0]:\n",
    "                            continue\n",
    "                    result = attack(img_id, model, target, pixel_count, \n",
    "                                maxiter, popsize, verbose, attack_img=True)\n",
    "                    img = Image.fromarray(result[10])\n",
    "                    img.save('file path!!', 'png')\n",
    "                    model_results.append(result[:-1])\n",
    "                    \n",
    "[]:-1 += model_results\n",
    "        results += model_resultsint(results, targeted)\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "untargeted = attack_all(models, maxiter=30, current_id=21715, targeted=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "targeted = attack_all(models, samples=10, targeted=True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Attack Statistics"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Print the final results! "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the results\n",
    "untargeted, targeted = helper.load_results()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "columns = ['model', 'pixels', 'image', 'true', 'predicted', 'success', 'cdiff', 'prior_probs', 'predicted_probs', 'perturbation']\n",
    "untargeted_results = pd.DataFrame(untargeted, columns=columns)\n",
    "targeted_results = pd.DataFrame(targeted, columns=columns)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Untargeted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "helper.attack_stats(untargeted_results, models, network_stats)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Targeted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "helper.attack_stats(targeted_results, models, network_stats)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Show some successful attacks"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot 9 random successful attack images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Untargeted Attack')\n",
    "helper.visualize_attack(untargeted_results, class_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Targeted Attack')\n",
    "helper.visualize_attack(targeted_results, class_names)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusions"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It appears that the accuracy of a model is not strongly correlated with the chance of performing a successful attack on an image. Perhaps surprisingly, the purely convolutional model is the most resistant CNN to these types of attacks. In addition, the capsule network CapsNet has the lowest attack success rate out of all the models, although it is still vulnerable to attack.\n",
    "\n",
    "[Part 2](1_one-pixel-attack-cifar10.ipynb)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Credits\n",
    " -  This implemenation is based off of the original paper describing the one pixel attack: https://arxiv.org/abs/1710.08864\n",
    " - Base code for iPython notebook: https://github.com/09rohanchopra/cifar10\n",
    " - Keras Cifar10 models: https://github.com/BIGBALLON/cifar-10-cnn\n",
    " - Scipy's differential evolution implementation: https://docs.scipy.org/doc/scipy-0.17.0/reference/generated/scipy.optimize.differential_evolution.html\n",
    " - State of the art: https://github.com/RedditSota/state-of-the-art-result-for-machine-learning-problems\n",
    " - CapsNet Keras: https://github.com/XifengGuo/CapsNet-Keras\n",
    " - CapsNet with Cifar: https://github.com/theblackcat102/dynamic-routing-capsule-cifar"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
